\section{引言}
随着人工智能技术的普及，数据驱动的方法正加速融入科研与产业流程之中，成为推动科学发现的重要引擎。然而，主流深度学习模型普遍呈现“黑箱”特征，其内部决策逻辑难以解释与验证，这在物理规律发现、医学诊断及工程建模等对可靠性要求极高的场景中构成显著信任壁垒。在此背景下，符号回归（Symbolic Regression, SR）作为一种能够从数据中直接反演数学解析表达式的技术，近年来受到了广泛关注。与传统回归分析不同，符号回归不预设函数形式，而是致力于搜索能够精准描述数据及其内在机制的显式方程。这类方程具有良好的可解释性与可迁移性，并可作为可检验的结构化假设，帮助研究人员揭示数据背后的物理或动力学规律。该技术已在天体物理探索、电池寿命预测以及复杂系统控制等领域展现出应用潜力。随着应用扩展，符号回归的方法谱系、数据基准与评测协议也在近两年被系统梳理与标准化：综述工作总结了进化式、神经式与混合式路线及其评测要点\cite{zhong2025survey}，而 SRBench++ 进一步引入更细粒度子任务与专家可解释性评估以提升基准可信度\cite{defranca2024srbenchpp}；同时也有研究反思 Feynman 系列数据的采样与数值误差指标局限，并倡导使用结构相似度（如归一化编辑距离）度量更贴近科学发现需求\cite{matsubara2024rethinking}。

尽管应用前景广阔，符号回归在实际落地中仍面临严峻挑战。其核心难点在于数学表达式空间的组合复杂性：随着表达式长度的增加和算子集合的扩展，候选解空间呈指数级爆炸增长。此外，寻找正确的数学骨架（结构搜索）与优化常数参数（系数拟合）高度耦合，使得符号回归在一般情形下表现为计算极其密集的 NP 难问题。如何在浩瀚且非凸的搜索空间中高效、稳健地收敛到符合数据分布的最优解，始终是该领域亟待攻克的难题。

针对上述挑战，现有解决方案主要分为基于遗传编程的方法与基于深度神经网络的方法。基于遗传编程（Genetic Programming, GP）的方法是符号回归领域的传统范式，最早由 Koza 等人提出\cite{koza1990}. 该类方法使用树状结构表示表达式，通过选择、交叉与变异等进化算子在种群中迭代搜索。近年来，Cranmer 等人提出的 PySR 框架将遗传编程与高性能分布式计算结合，在一定程度上提升了搜索效率\cite{cranmer2023pysr}. 然而，GP 方法通常对超参数高度敏感，且在处理高维数据时容易过早收敛于局部最优解。为克服这些局限，基于预训练模型的方法应运而生。Biggio 等人\cite{biggio2021nsr}与 Kamienny 等人\cite{kamienny2022e2e}将符号回归建模为序列生成任务，利用 Transformer 在大规模合成数据集上进行预训练。这类方法推理速度快且具备良好泛化能力，但大多采用自回归生成模式，即一次性从左至右生成表达式，缺乏对生成过程中错误的回溯或修正机制，一旦初始步骤出现偏差，后续往往难以纠正。除预训练生成模型外，近期也出现了利用大语言模型进行“生成—验证—修正”的方程发现范式：ICSR 通过让 LLM 迭代改写函数形式并由外部优化器拟合系数\cite{merler2024icsr}，LLM-SR 则把方程发现视为程序生成并结合进化式搜索与参数优化\cite{shojaee2025llmsr}，而 SR-LLM 进一步将检索增强生成用于增量式知识复用与跨任务迁移\cite{guo2025srllm}。尽管后续研究尝试引入蒙特卡洛树搜索（MCTS）\cite{shojaee2023planning}、最小描述长度原则（MDL）\cite{yu2024mdlformer}或离线强化学习机制\cite{tian2025interactive}以增强搜索能力，但在高噪声数据或复杂结构场景下，训练稳定性与结果鲁棒性仍有待提升。

为了兼顾深度学习的泛化能力与传统搜索算法的迭代优化特性，受 Levenshtein Transformer\cite{gu2019levenshtein}、分子编辑模型\cite{zheng2024smieditor}以及流匹配思想\cite{havasi2025editflows}的启发，本文基于 Edit Flows 思路，将其应用于符号回归并形成 EditFlow 框架。我们将符号回归重新构想为离散编辑流（Discrete Edit Flow）过程：从一个初始状态（如随机表达式或简单变量）出发，通过一系列插入、删除与替换操作，逐步将当前表达式编辑为能够精准拟合数据的目标形式。

本文的核心贡献主要体现在以下三个方面：
\begin{itemize}
  \item 基于 LLaMA 构建编辑流骨干网络。我们设计了定制化的 LlamaEditFlowBackbone，打破标准语言模型的自回归范式。通过引入多头输出机制（分别预测操作类型概率与词汇分布）与交叉注意力注入（Cross-Attention Injection），模型能够根据观测数据在表达式树上预测节点级最优编辑动作。
  \item 设计基于 SetTransformer 的条件编码机制。该模块将输入变量与目标值对 $(\mathbf{X}, \mathbf{y}_{target})$ 编码为具有排列不变性的条件向量，作为推理过程中的“北极星”持续指引编辑方向，确保表达式演化路径收敛于真实数据分布。
  \item 构建迭代式贪婪搜索推理算法。推理阶段通过多步离散编辑操作逐步修正表达式结构并优化常数，配合基于 Levenshtein 距离与随机化对齐（Randomized Alignment）的训练策略，缓解离散序列在连续编辑空间中的对齐困难，提升复杂方程搜索成功率。
\end{itemize}
